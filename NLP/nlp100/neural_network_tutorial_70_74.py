# %%
# -*- coding: utf-8 -*-
"""neural_network_tutorial_70_74.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1sxA7AiJ-g1BitIL0YfzLpUY8VWskHpFK

# Neural Network

This tutorial references NLP100knock (
[English page](https://nlp100.github.io/en/ch08.html) / [Japanese page](https://nlp100.github.io/ja/ch08.html) / [Chinese page](https://nlp100.github.io/zh/ch08.html)).

## üö® Before you start running

- You can effectively use ChatGPT (or some other AI tools) to work on the tutorials. You practically use it while conducting your research.
- If you have no idea how to solve those exercise, you can refer to sample answers by others on the internet. Thoroughly examining better code from many example answers and manually copying them is a good way to improve your coding skills.

## 70. Generating Features through Word Vector Summation

See https://nlp100.github.io/en/ch08.html#70-generating-features-through-word-vector-summation

### Let's move back 50 to get the files

```
Download News Aggregator Data Set and create training data (train.txt), validation data (valid.txt) and test data (test.txt) as follows:

Unpack the downloaded zip file and read readme.txt.
Extract the articles such that the publisher is one of the followings: ‚ÄúReuters‚Äù, ‚ÄúHuffington Post‚Äù, ‚ÄúBusinessweek‚Äù, ‚ÄúContactmusic.com‚Äù and ‚ÄúDaily Mail‚Äù.
Randomly shuffle the extracted articles.
Split the extracted articles in the following ratio: the training data (80%), the validation data (10%) and the test data (10%). Then save them into files train.txt, valid.txt and test.txt, respectively. In each file, each line should contain a single instance. Each instance should contain both the name of the category and the article headline. Use Tab to separate each field.
After creating the dataset, check the number of instances contained in each category.
```
"""

import pandas as pd

# Download data and unzip it
!mkdir -p 7
!wget https://archive.ics.uci.edu/static/public/359/news+aggregator.zip -O 7/news+aggregator.zip
!unzip -o "./7/news+aggregator.zip" -d "./7/"
!rm -rf "./7/__MACOSX"
# %%
"""Note (Just my thought process):

- This task needs:
  `Extract the articles such that the publisher is one of the followings...`
- Recommend you to read readme.txt in detail.
  - The file readme.txt, shows where "publisher" column is in and tell us the file we need to load.

  - The file also shows the file format, so we can get the information about the deliminer that splits the elements. This indicates what should be input for the `sep` parameter of the `read_csv` method.

  - The file doesn't have header so we need some parameter to avoid header while loading the file or specify the header (using the param `names' in read_csv or df.columns after loading the file).
"""

# Load data
df = pd.read_csv("7/newsCorpora.csv", sep="\t", header=None)
# Extract the articles such that the publisher is one of the followings: ‚ÄúReuters‚Äù, ‚ÄúHuffington Post‚Äù, ‚ÄúBusinessweek‚Äù, ‚ÄúContactmusic.com‚Äù and ‚ÄúDaily Mail‚Äù.
extracted_df = df[df[3].isin(["Reuters", "Huffington Post", "Businessweek", "Contactmusic.com", "Daily Mail"])]
# Randomly shuffle the extracted articles.
shuffled_df = extracted_df.sample(frac=1, random_state=0)

print(shuffled_df.head())
# %%
from sklearn.model_selection import train_test_split
# Split the extracted articles in the following ratio: the training data (80%), the validation data (10%) and the test data (10%).
# train_test_split is an easy way to shuffle and split data
# Use like this: train_df, test_df = train_test_split(full_df, test_size=0.4)
    #  to get 60% of the full_df in train_df and 40% in test_df

train_df, valid_test_df = train_test_split(shuffled_df, test_size=0.2, random_state=0)
valid_df, test_df = train_test_split(valid_test_df, test_size=0.5, random_state=0)

assert len(train_df) == 10672
assert len(valid_df) == 1334
assert len(test_df)  == 1334
# %%
# Then save them into files train.txt, valid.txt and test.txt, respectively.
# In each file, each line should contain a single instance.
# Each instance should contain both the name of the category and the article headline.
# Use *Tab* to separate each field.
train_df.to_csv("7/train.txt", index=None, header=None)
valid_df.to_csv("7/valid.txt", index=None, header=None)
test_df.to_csv("7/test.txt", index=None, header=None)
# %% 
# Refresh your folder to make sure the files were created

"""### Let' move back to 60 to get the vectors
Download word vectors that are pretrained on Google News dataset (approx. 100 billion words). The file contains word vectors of 3 million words/phrases, whose dimentionalities are 300.
"""

# If you have not had the vector data yet. You can download locally or with gdown.
# *"0B7XkCwpI5KDYNlNUTTlSS21pQmM" indicates the id finding in the download URL https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/view?usp=sharing&resourcekey=0-wjGZdNAUop6WykTtMip30g
# !pip install gdown
# !gdown -O "./7/" "https://drive.google.com/uc?id=0B7XkCwpI5KDYNlNUTTlSS21pQmM"
# !gzip -d ./7/GoogleNews-vectors-negative300.bin.gz
# %%

from gensim.models import KeyedVectors
model = KeyedVectors.load_word2vec_format("7/GoogleNews-vectors-negative300.bin", binary=True)
# %%
"""### Try 70
Note: You can use only the train:1000, valid:100, test:100 data because it takes long time to get vectors.
"""

# Load data
columns = ["ID", "TITLE", "URL", "PUBLISHER", "CATEGORY", "STORY", "HOSTNAME", "TIMESTAMP"]
train_df = pd.read_csv("7/train.txt", header=None, names=columns)
valid_df = pd.read_csv("7/valid.txt", header=None, names=columns)
test_df = pd.read_csv("7/test.txt", header=None, names=columns)

# We only use these two columns
train_df = train_df[["TITLE", "CATEGORY"]]
valid_df = valid_df[["TITLE", "CATEGORY"]]
test_df = test_df[["TITLE", "CATEGORY"]]

# %%

# Replace category name to label index. (See readme.txt)
category2label = lambda x: ["b", "t", "e", "m"].index(x)
train_df['CATEGORY'] = train_df['CATEGORY'].map(category2label)
valid_df['CATEGORY'] = valid_df['CATEGORY'].map(category2label)
test_df['CATEGORY'] = test_df['CATEGORY'].map(category2label)
# %%
# Save data
train_df.to_csv("7/y_train.txt", header=False, columns=["CATEGORY"], index=None)
valid_df.to_csv("7/y_valid.txt", header=False, columns=["CATEGORY"], index=None)
test_df.to_csv("7/y_test.txt", header=False, columns=["CATEGORY"], index=None)
# %%
import numpy as np
from tqdm import tqdm

def make_vectors(df, length = 1000):
  all_vectors = []
  for title in tqdm(df["TITLE"].tolist()[:length]):
    vectors = []
    words = title.lower().split()
    for word in words:
      # Note if the word is not in the model.index_to_key, skip it.
      if word not in model.index_to_key:
        continue
      vector = model[word] # Get vectors from loaded model
      vectors.append(vector)
    
    vectors = np.array(vectors)
    if len(vectors) == 0:
      mean_vector = np.zeros(300)
    else:
      mean_vector = np.mean(vectors, axis=0) # Mean value per column (See https://numpy.org/doc/stable/reference/generated/numpy.mean.html)
    
    assert mean_vector.shape == (300,)
    all_vectors.append(mean_vector)
  all_vectors = np.array(all_vectors)
  assert all_vectors.shape == (length, 300)
  return all_vectors


with open('7/x_train.npy', 'wb') as f:
  np.save(f, make_vectors(train_df))

with open('7/x_valid.npy', 'wb') as f:
  np.save(f, make_vectors(valid_df))

with open('7/x_test.npy', 'wb') as f:
  np.save(f, make_vectors(test_df))



# Get vectors for validation and test data each here!!!
# %%
"""## 71. Building Single Layer Neural Network
See https://nlp100.github.io/en/ch08.html#71-building-single-layer-neural-network
"""

import torch

# Create randomized weights (dim, the number of labels) https://pytorch.org/docs/stable/generated/torch.rand.html#torch-rand
W = torch.rand(300, 1000)
with open("7/x_train.npy", "rb") as f:
  x_train = np.load(f)
  x_train = torch.tensor(x_train, dtype=torch.float32)

# Note: the shape x_train and W are required same dimension
assert x_train.dtype == W.dtype == torch.float32
print(x_train.shape)
# %%
# Prepare softmax https://pytorch.org/docs/stable/generated/torch.nn.functional.softmax.html#torch-nn-functional-softmax
import torch.nn as nn
softmax = nn.Softmax(dim=1)

# Calculate Matrix product of x_train[:1] and W ‚Üí Calculate its softmax
print(softmax(x_train[:1] @ W).shape)

# Calculate Matrix product of x_train[:4] and W ‚Üí Calculate its softmax
print(softmax(x_train[:4] @ W).shape)
# %%
"""## 72. Calculating loss and gradients
See https://nlp100.github.io/en/ch08.html#72-calculating-loss-and-gradients
"""

net = nn.Linear(300, 4, bias=False) # Prepare a linear layer without *bias*. The shape is (dim, the number of labels) (See https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear)
loss = nn.CrossEntropyLoss() # Prepare CrossEntropyLoss (See https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss)

with open("7/x_train.npy", "rb") as f:
  x_train = np.load(f)
  x_train = torch.tensor(x_train, dtype=torch.float32) # Convert numpy to tensor
with open("7/y_train.txt") as f:
  y_train = [int(line) for line in f.readlines()] # Load data and convert str to int
  y_train = torch.tensor(y_train, dtype=torch.long) # Convert numpy to tensor. Note: the datatype of y_train is not float tensor but int.

print(x_train.shape, x_train[:4].shape)
# %%
# Calculate Matrix product of x_train[:1] and net (Hint: net.forward(x))‚Üí Calculate its softmax
y_pred1 = softmax(net(x_train[:1]))
# Calculate Matrix product of x_train[:1] and net (Hint: net.forward(x)) ‚Üí Calculate its softmax
y_pred4 = softmax(net(x_train[:4]))
print(y_pred1.shape, y_pred4.shape)
# %%
loss1 = loss(y_pred1, y_train[:1])
net.zero_grad() # Initialize the gradient
loss1.backward() # Calculate backword()
print(f"x1 CrossEntropyloss: {loss1}")
print(f"x1 Gradient: {net.weight.grad}")

loss4 = loss(y_pred4, y_train[:4])
net.zero_grad() # Initialize the gradient
loss4.backward() # Calculate backword()
print(f"x1-x4 CrossEntropyloss: {loss4}")
print(f"x1 Gradient: {net.weight.grad}")
# %%
"""## 73. Learning with stochastic gradient descent

See https://nlp100.github.io/en/ch08.html#72-calculating-loss-and-gradients
"""

net = nn.Sequential(
  nn.Linear(300, 256), 
  nn.ReLU(),
  nn.Linear(256, 64),
  nn.ReLU(),
  nn.Linear(64, 4)
) # Prepare a linear layer without *bias*. The shape is (dim, the number of labels) (See https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear)
loss_function = nn.CrossEntropyLoss() # Prepare CrossEntropyLoss (See https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss)
optimizer = torch.optim.SGD(net.parameters(), lr=0.01) # Prepare optimizer (See https://pytorch.org/docs/stable/generated/torch.optim.SGD.html#torch.optim.SGD)

print(x_train.shape, y_train.shape)
x_train, y_train = x_train[:1000], y_train[:1000]
# %%
losses = []
for epoch in tqdm(range(100000)):
    optimizer.zero_grad() # Initialize the gradient
    y_pred = net(x_train)
    # Calculate Matrix product of x_train and net (Hint: net.forward(x))‚Üí Calculate its softmax
    loss = loss_function(y_pred, y_train) # Calculate loss
    loss.backward()
    optimizer.step()
    # Calculate backword here (See example in https://pytorch.org/docs/stable/generated/torch.optim.SGD.html#torch.optim.SGD)
    # Run optimizer step here (See example in https://pytorch.org/docs/stable/generated/torch.optim.SGD.html#torch.optim.SGD)
    losses.append(loss)
# Save the weights
print(losses[0], losses[-1])
torch.save(net.state_dict(), "7/model.pt")
# %%
"""## 74. Measuring accuracy

See https://nlp100.github.io/en/ch08.html#72-calculating-loss-and-gradients
"""

# net = nn.Linear(300, 4)  # Prepare a linear layer without *bias*. The shape is (dim, the number of labels) (See https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear)
# # Load model
# net.load_state_dict(torch.load("7/model.pt"))
# %%
with open("7/x_test.npy", "rb") as f:
  # Load numpy and convert it to tensor
  x_test = np.load(f)
  x_test = torch.tensor(x_test, dtype=torch.float32)

with open("7/y_test.txt") as f:
  y_test = [int(line) for line in f.readlines()] # Load data and convert str to int
  y_test = torch.tensor(y_test, dtype=torch.long)
  # Load numpy and convert it to tensor
print(x_test.shape, y_test.shape)

x_test, y_test = x_test[:100], y_test[:100]
# %%

from sklearn.metrics import accuracy_score
y_max_train, y_pred_train = torch.max(net(x_train), dim=1) # Check the return of torch.max (See https://pytorch.org/docs/stable/generated/torch.max.html#torch-max)
print(y_train.shape, y_pred_train.shape)
# %%
print(f"Accuracy on training data: {accuracy_score(y_train, y_pred_train):.2f}")
# See https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html#sklearn.metrics.accuracy_score

y_max_test, y_pred_test = torch.max(net(x_test),dim=1)
print(f"Accuraty on test data: {accuracy_score(y_test, y_pred_test):.2f}")
# %%
