# %%
# -*- coding: utf-8 -*-
"""neural_network_tutorial_70_74.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1sxA7AiJ-g1BitIL0YfzLpUY8VWskHpFK

# Neural Network

This tutorial references NLP100knock (
[English page](https://nlp100.github.io/en/ch08.html) / [Japanese page](https://nlp100.github.io/ja/ch08.html) / [Chinese page](https://nlp100.github.io/zh/ch08.html)).

## ðŸš¨ Before you start running

- You can effectively use ChatGPT (or some other AI tools) to work on the tutorials. You practically use it while conducting your research.
- If you have no idea how to solve those exercise, you can refer to sample answers by others on the internet. Thoroughly examining better code from many example answers and manually copying them is a good way to improve your coding skills.

## 70. Generating Features through Word Vector Summation

See https://nlp100.github.io/en/ch08.html#70-generating-features-through-word-vector-summation

### Let's move back 50 to get the files

```
Download News Aggregator Data Set and create training data (train.txt), validation data (valid.txt) and test data (test.txt) as follows:

Unpack the downloaded zip file and read readme.txt.
Extract the articles such that the publisher is one of the followings: â€œReutersâ€, â€œHuffington Postâ€, â€œBusinessweekâ€, â€œContactmusic.comâ€ and â€œDaily Mailâ€.
Randomly shuffle the extracted articles.
Split the extracted articles in the following ratio: the training data (80%), the validation data (10%) and the test data (10%). Then save them into files train.txt, valid.txt and test.txt, respectively. In each file, each line should contain a single instance. Each instance should contain both the name of the category and the article headline. Use Tab to separate each field.
After creating the dataset, check the number of instances contained in each category.
```
"""

import pandas as pd

# Download data and unzip it
!mkdir -p 7
!wget https://archive.ics.uci.edu/static/public/359/news+aggregator.zip -O 7/news+aggregator.zip
!unzip -o "./7/news+aggregator.zip" -d "./7/"
!rm -rf "./7/__MACOSX"
# %%
"""Note (Just my thought process):

- This task needs:
  `Extract the articles such that the publisher is one of the followings...`
- Recommend you to read readme.txt in detail.
  - The file readme.txt, shows where "publisher" column is in and tell us the file we need to load.

  - The file also shows the file format, so we can get the information about the deliminer that splits the elements. This indicates what should be input for the `sep` parameter of the `read_csv` method.

  - The file doesn't have header so we need some parameter to avoid header while loading the file or specify the header (using the param `names' in read_csv or df.columns after loading the file).
"""

# Load data
df = pd.read_csv("7/newsCorpora.csv", sep="\t", header=None)
# Extract the articles such that the publisher is one of the followings: â€œReutersâ€, â€œHuffington Postâ€, â€œBusinessweekâ€, â€œContactmusic.comâ€ and â€œDaily Mailâ€.
extracted_df = df[df[3].isin(["Reuters", "Huffington Post", "Businessweek", "Contactmusic.com", "Daily Mail"])]
# Randomly shuffle the extracted articles.
shuffled_df = extracted_df.sample(frac=1, random_state=0)

print(shuffled_df.head())
# %%
from sklearn.model_selection import train_test_split
# Split the extracted articles in the following ratio: the training data (80%), the validation data (10%) and the test data (10%).
# train_test_split is an easy way to shuffle and split data
# Use like this: train_df, test_df = train_test_split(full_df, test_size=0.4)
    #  to get 60% of the full_df in train_df and 40% in test_df

train_df, valid_test_df = train_test_split(shuffled_df, test_size=0.2, random_state=0)
valid_df, test_df = train_test_split(valid_test_df, test_size=0.5, random_state=0)

assert len(train_df) == 10672
assert len(valid_df) == 1334
assert len(test_df)  == 1334
# %%
# Then save them into files train.txt, valid.txt and test.txt, respectively.
# In each file, each line should contain a single instance.
# Each instance should contain both the name of the category and the article headline.
# Use *Tab* to separate each field.
train_df.to_csv("7/train.txt", index=None, header=None)
valid_df.to_csv("7/valid.txt", index=None, header=None)
test_df.to_csv("7/test.txt", index=None, header=None)
# %% 
# Refresh your folder to make sure the files were created

"""### Let' move back to 60 to get the vectors
Download word vectors that are pretrained on Google News dataset (approx. 100 billion words). The file contains word vectors of 3 million words/phrases, whose dimentionalities are 300.
"""

# If you have not had the vector data yet. You can download locally or with gdown.
# *"0B7XkCwpI5KDYNlNUTTlSS21pQmM" indicates the id finding in the download URL https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/view?usp=sharing&resourcekey=0-wjGZdNAUop6WykTtMip30g
# !pip install gdown
# !gdown -O "./7/" "https://drive.google.com/uc?id=0B7XkCwpI5KDYNlNUTTlSS21pQmM"
# !gzip -d ./7/GoogleNews-vectors-negative300.bin.gz
# %%

from gensim.models import KeyedVectors
model = KeyedVectors.load_word2vec_format("7/GoogleNews-vectors-negative300.bin", binary=True)
# %%
"""### Try 70
Note: You can use only the train:1000, valid:100, test:100 data because it takes long time to get vectors.
"""

# Load data
columns = ["ID", "TITLE", "URL", "PUBLISHER", "CATEGORY", "STORY", "HOSTNAME", "TIMESTAMP"]
train_df = pd.read_csv("7/train.txt", header=None, names=columns)
valid_df = pd.read_csv("7/valid.txt", header=None, names=columns)
test_df = pd.read_csv("7/test.txt", header=None, names=columns)

# We only use these two columns
train_df = train_df[["TITLE", "CATEGORY"]]
valid_df = valid_df[["TITLE", "CATEGORY"]]
test_df = test_df[["TITLE", "CATEGORY"]]

# %%

# Replace category name to label index. (See readme.txt)
category2label = lambda x: ["b", "t", "e", "m"].index(x)
train_df['CATEGORY'] = train_df['CATEGORY'].map(category2label)
valid_df['CATEGORY'] = valid_df['CATEGORY'].map(category2label)
test_df['CATEGORY'] = test_df['CATEGORY'].map(category2label)
# %%
# Save data
train_df.to_csv("7/y_train.txt", header=False, columns=["CATEGORY"], index=None)
valid_df.to_csv("7/y_valid.txt", header=False, columns=["CATEGORY"], index=None)
test_df.to_csv("7/y_test.txt", header=False, columns=["CATEGORY"], index=None)
# %%
import numpy as np
from tqdm import tqdm

def make_vectors(df, length = 1000):
  all_vectors = []
  for title in tqdm(df["TITLE"].tolist()[:length]):
    vectors = []
    words = title.lower().split()
    for word in words:
      # Note if the word is not in the model.index_to_key, skip it.
      if word not in model.index_to_key:
        continue
      vector = model[word] # Get vectors from loaded model
      vectors.append(vector)
    
    vectors = np.array(vectors)
    if len(vectors) == 0:
      mean_vector = np.zeros(300)
    else:
      mean_vector = np.mean(vectors, axis=0) # Mean value per column (See https://numpy.org/doc/stable/reference/generated/numpy.mean.html)
    
    assert mean_vector.shape == (300,)
    all_vectors.append(mean_vector)
  all_vectors = np.array(all_vectors)
  assert all_vectors.shape == (length, 300)
  return all_vectors

if not Path("7/x_train.npy").exists():
  with open('7/x_train.npy', 'wb') as f:
    np.save(f, make_vectors(train_df))

if not Path("7/x_valid.npy").exists():
  with open('7/x_valid.npy', 'wb') as f:
    np.save(f, make_vectors(valid_df))

if not Path("7/x_test.npy").exists():
  with open('7/x_test.npy', 'wb') as f:
    np.save(f, make_vectors(test_df))



# Get vectors for validation and test data each here!!!
# %%
"""## 71. Building Single Layer Neural Network
See https://nlp100.github.io/en/ch08.html#71-building-single-layer-neural-network
"""

import torch

# Create randomized weights (dim, the number of labels) https://pytorch.org/docs/stable/generated/torch.rand.html#torch-rand
W = torch.rand(300, 1000)
with open("7/x_train.npy", "rb") as f:
  x_train = np.load(f)
  x_train = torch.tensor(x_train, dtype=torch.float32)

# Note: the shape x_train and W are required same dimension
assert x_train.dtype == W.dtype == torch.float32
print(x_train.shape)
# %%
# Prepare softmax https://pytorch.org/docs/stable/generated/torch.nn.functional.softmax.html#torch-nn-functional-softmax
import torch.nn as nn
softmax = nn.Softmax(dim=1)

# Calculate Matrix product of x_train[:1] and W â†’ Calculate its softmax
print(softmax(x_train[:1] @ W).shape)

# Calculate Matrix product of x_train[:4] and W â†’ Calculate its softmax
print(softmax(x_train[:4] @ W).shape)
# %%
"""## 72. Calculating loss and gradients
See https://nlp100.github.io/en/ch08.html#72-calculating-loss-and-gradients
"""

net = nn.Linear(300, 4, bias=False) # Prepare a linear layer without *bias*. The shape is (dim, the number of labels) (See https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear)
loss = nn.CrossEntropyLoss() # Prepare CrossEntropyLoss (See https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss)

with open("7/x_train.npy", "rb") as f:
  x_train = np.load(f)
  x_train = torch.tensor(x_train, dtype=torch.float32) # Convert numpy to tensor
with open("7/y_train.txt") as f:
  y_train = [int(line) for line in f.readlines()] # Load data and convert str to int
  y_train = torch.tensor(y_train, dtype=torch.long) # Convert numpy to tensor. Note: the datatype of y_train is not float tensor but int.

print(x_train.shape, x_train[:4].shape)
# %%
# Calculate Matrix product of x_train[:1] and net (Hint: net.forward(x))â†’ Calculate its softmax
y_pred1 = softmax(net(x_train[:1]))
# Calculate Matrix product of x_train[:1] and net (Hint: net.forward(x)) â†’ Calculate its softmax
y_pred4 = softmax(net(x_train[:4]))
print(y_pred1.shape, y_pred4.shape)
# %%
loss1 = loss(y_pred1, y_train[:1])
net.zero_grad() # Initialize the gradient
loss1.backward() # Calculate backword()
print(f"x1 CrossEntropyloss: {loss1}")
print(f"x1 Gradient: {net.weight.grad}")

loss4 = loss(y_pred4, y_train[:4])
net.zero_grad() # Initialize the gradient
loss4.backward() # Calculate backword()
print(f"x1-x4 CrossEntropyloss: {loss4}")
print(f"x1 Gradient: {net.weight.grad}")
# %%
"""## 73. Learning with stochastic gradient descent

See https://nlp100.github.io/en/ch08.html#72-calculating-loss-and-gradients
"""

net = nn.Linear(300, 4, bias=False)

# net = nn.Sequential(
#     nn.Linear(300, 512),
#     nn.ReLU(),
#     nn.Linear(512, 256),
#     nn.ReLU(),
#     nn.Linear(256, 64),
#     nn.ReLU(),
#     nn.Linear(64, 4),
# )

# Prepare a linear layer without *bias*. The shape is (dim, the number of labels) (See https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear)
loss_function = nn.CrossEntropyLoss() # Prepare CrossEntropyLoss (See https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss)
optimizer = torch.optim.SGD(net.parameters(), lr=0.01) # Prepare optimizer (See https://pytorch.org/docs/stable/generated/torch.optim.SGD.html#torch.optim.SGD)

print(x_train.shape, y_train.shape)
# %%
# x_train, y_train = x_train[:1000], y_train[:1000]

from sklearn.metrics import accuracy_score
import matplotlib.pyplot as plt
with open("7/x_test.npy", "rb") as f:
  # Load numpy and convert it to tensor
  x_test = np.load(f)
  x_test = torch.tensor(x_test, dtype=torch.float32)

with open("7/y_test.txt") as f:
  y_test = [int(line) for line in f.readlines()] # Load data and convert str to int
  y_test = torch.tensor(y_test, dtype=torch.long)
  # Load numpy and convert it to tensor
print(x_test.shape, y_test.shape)

x_test, y_test = x_test[:100], y_test[:100]

x_train, y_train = x_train.to("cuda"), y_train.to("cuda")
x_test, y_test = x_test.to("cuda"), y_test.to("cuda")
net.to("cuda")
losses = []
accuracies = []
for epoch in tqdm(range(10000)):
    optimizer.zero_grad() # Initialize the gradient
    y_pred = net(x_train)
    # Calculate Matrix product of x_train and net (Hint: net.forward(x))â†’ Calculate its softmax
    loss = loss_function(y_pred, y_train) # Calculate loss
    loss.backward()
    optimizer.step()
    # Calculate backword here (See example in https://pytorch.org/docs/stable/generated/torch.optim.SGD.html#torch.optim.SGD)
    # Run optimizer step here (See example in https://pytorch.org/docs/stable/generated/torch.optim.SGD.html#torch.optim.SGD)
    losses.append(loss.item())
    net.eval()
    with torch.inference_mode():
      y_max_test, y_pred_test = torch.max(net(x_test), dim=1)
      acc = accuracy_score(y_test.cpu(), y_pred_test.cpu())
    accuracies.append(acc)
    net.train()
# Save the weights
print(losses[0], losses[-1])

fig = plt.figure()
ax1 = fig.add_subplot(111)
ax1.plot(losses, label="loss")
ax1.set_ylim(0, 2)
ax1.set_ylabel("loss")

ax2 = ax1.twinx()
ax2.plot(accuracies, label="accuracy", color="orange")
ax2.set_ylim(0, 1)
ax2.set_ylabel("accuracy")

plt.show()

torch.save(net.state_dict(), "7/model.pt")
# %%
"""## 74. Measuring accuracy

See https://nlp100.github.io/en/ch08.html#72-calculating-loss-and-gradients
"""

# net = nn.Linear(300, 4)  # Prepare a linear layer without *bias*. The shape is (dim, the number of labels) (See https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear)
# # Load model
# net.load_state_dict(torch.load("7/model.pt"))
# %%
with open("7/x_test.npy", "rb") as f:
  # Load numpy and convert it to tensor
  x_test = np.load(f)
  x_test = torch.tensor(x_test, dtype=torch.float32)

with open("7/y_test.txt") as f:
  y_test = [int(line) for line in f.readlines()] # Load data and convert str to int
  y_test = torch.tensor(y_test, dtype=torch.long)
  # Load numpy and convert it to tensor
print(x_test.shape, y_test.shape)

x_test, y_test = x_test[:100], y_test[:100]
# %%

from sklearn.metrics import accuracy_score
y_max_train, y_pred_train = torch.max(net(x_train), dim=1) # Check the return of torch.max (See https://pytorch.org/docs/stable/generated/torch.max.html#torch-max)
print(y_train.shape, y_pred_train.shape)
# %%
print(f"Accuracy on training data: {accuracy_score(y_train, y_pred_train):.2f}")
# See https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html#sklearn.metrics.accuracy_score

y_max_test, y_pred_test = torch.max(net(x_test),dim=1)
print(f"Accuraty on test data: {accuracy_score(y_test, y_pred_test):.2f}")
# %%

"""
# Neural Network

This tutorial references NLP100knock (
[English page](https://nlp100.github.io/en/ch08.html) / [Japanese page](https://nlp100.github.io/ja/ch08.html) / [Chinese page](https://nlp100.github.io/zh/ch08.html)).

## ðŸš¨ Before you start running

- Please use codes from previsou(70-74) lessons.
"""

# 75. Plotting loss and accuracy
"""Modify the code from the problem 73 so that the loss and accuracy of both the training and the evaluation data are plotted on a graph after each epoch. Use this graph to monitor the progress of learning

Paste your 73. code and modify it here.
Please plot the graph of
  - loss
  -  accuracy
for both training and evaluation data.
"""

# %%
# 76. Checkpoints
"""Modify the code from the problem 75 to write out checkpoints to a file after each epoch. Checkpoints should include values of the parameters such as weight matrices and the internal states of the optimization algorithm.

Paste your 75. code and modify it here.
Add the code to write out checkpoints to a file after each epoch.
"""

# %%
# 77. Mini-batches
"""
Modify the code from the problem 76 to calculate the loss/gradient and update the values of matrix W
 for every B
 samples (mini-batch). Compare the time required for one learning epoch by changing the value of B
 to 1,2,4,8,â€¦
.
Paste your 76. code and modify it here.
Make the code possible to get mini-batches data.
Pytorch APIs basically allows batch dimensions in the first dimention by default.(See the example https://pytorch.org/docs/stable/generated/torch.bmm.html#torch-bmm)
You can set the batch size (first dimention) as variable B.
"""

# %%
# 78 Training on a GPU
"""Modify the code from the problem 77 so that it runs on a GPU.

Paste your 77. code and modify it here.
Use GPU instances in Google Colab.
- Runtime -> Change runtime type -> Hardware accelerator ->T4 GPU

Check GPU avaiability:
>>> import torch
>>> torch.cuda.is_available()

Example code to change model to GPU: https://pytorch.org/tutorials/recipes/recipes/save_load_across_devices.html#save-on-gpu-load-on-gpu
"""

# %%
# 79. Multilayer Neural Networks
"""
Modify the code from the problem 78 to create a high-performing classifier by changing the architecture of the neural network. Try introducing bias terms and multiple layers.

Paste your 78. code and modify it here.

Example for Linear with bias: https://pytorch.org/docs/stable/generated/torch.nn.Linear.html
Example for Multiple Layers: https://pytorch.org/tutorials/beginner/basics/buildmodel_tutorial.html#define-the-class
"""
# %%